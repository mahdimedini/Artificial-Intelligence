# -*- coding: utf-8 -*-
"""renfor (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GcpvFKSTLab6NQbn2yUef-0mRbtt-5w9
"""

!pip install deap
!pip install bitstring
from deap import base, creator, tools, algorithms
from scipy.stats import bernoulli
from bitstring import BitArray
from sklearn.feature_selection import mutual_info_classif
!pip install pandas
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import math
import sklearn
from keras.layers import Dropout
from keras.models import Sequential
from keras.layers import LSTM,Bidirectional
from keras.layers import Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import seaborn as sns

df=pd.read_csv('pollutionAndTrafficnew1.csv',parse_dates=True
                 ,index_col="timestamp")
df.index
df.head()
df.info()
print("shape =",df.shape)
df.describe()
df.tail()
sns.boxplot(x=df['vehicule_count'])
sns.boxplot(y=df['ozone'])

def findingout(data,variablename):
  iqr=data[variablename].quantile(0.75)-data[variablename].quantile(0.25)
  q1=data[variablename].quantile(0.25)
  q3=data[variablename].quantile(0.75)
  lower= q1 - iqr*1.5
  upper=q3 + iqr*1.5
  outdf=data[(data[variablename]<lower) | (data[variablename]>upper)]
  return(outdf)
#-----------------
print("max=",df['vehicule_count'].max())
print(" sum nul=",df['vehicule_count'].isna().sum())
#les val manquantes
df.fillna(df['vehicule_count'].quantile(0.75), inplace=True)
print(" sum nul apres remplissage=",df['vehicule_count'].isna().sum())
cols_to_normalize=['ozone','particullate_matter','carbon_monoxide','sulfure_dioxide','longitude','latitude']
df[cols_to_normalize] = df[cols_to_normalize].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
cols_to_normalize1=['nitrogen_dioxide','vehicule_count']
df[cols_to_normalize1] = df[cols_to_normalize1].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
#sns.distplot(df["vehicule_count"]);

plt.plot('longitude')
plt.show()
max=df['vehicule_count'].min()
print(max)
matrix = df.corr()
sns.heatmap(matrix, vmax=.8, square=True, cmap="Pastel2",annot=True,linewidths=.1);

df.drop("longitude",axis=1,inplace=True)
df.drop("latitude",axis=1,inplace=True)
print(df)

nf=1
print(df.iloc[:,5:6])
train_data = df[0:290]
test_data = df[290:]

def prepare(data,look_back,nf):
   train_x=[]
   train_y=[]


   for i in range(len(df) - look_back):
       train_x.append(df.iloc[i:i+look_back,0:df.shape[1]-1].values)
       train_y.append(df.iloc[i,5:6])



   return np.array(train_x),np.array(train_y),


from deap import base, creator, tools, algorithms
from scipy.stats import bernoulli
from bitstring import BitArray
from sklearn.model_selection import train_test_split as split
from keras.layers import LSTM, Input, Dense
from keras.models import Model

def train_evaluate(ga_individual_solution):
    # Decode GA solution to integer for window_size and num_units
    window_size_bits = BitArray(ga_individual_solution[0:6])
    num_units_bits = BitArray(ga_individual_solution[6:])
    window_size = window_size_bits.uint
    num_units = num_units_bits.uint
    print('\nWindow Size: ', window_size, ', Num of Units: ', num_units)

    # Return fitness score of 100 if window_size or num_unit is zero
    if window_size == 0 or num_units == 0:
        return 100,

    # Segment the train_data based on new window_size; split into train and validation (80/20)
    X,Y = prepare(df,window_size,1)


    X_train, X_val, y_train, y_val = split(X, Y, test_size = 0.20)

    import tensorflow as tf
    from tensorflow import keras

    model = Sequential()


    model.add(LSTM(num_units, input_shape=(X_train.shape[1],X_train.shape[2])))

    model.add(Dropout(0.2))
    model.add(Dense(1))

    import tensorflow as tf
    import keras.backend as K

    def root_mean_squared_error(y_true, y_pred):
         return K.sqrt(K.mean(K.square(y_pred - y_true)))

     #une couche de sortie qui fait une prédiction à valeur unique
    model.compile(loss=root_mean_squared_error, optimizer="adam",metrics=['accuracy'])
    history=model.fit(X_train, y_train, epochs=300, batch_size=10,verbose=2,validation_data=(X_val, y_val))

    from sklearn.metrics import mean_squared_error
    # make predictions
    #trainPredict = model.predict(X_test1)
    import math
    yhat = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, yhat))
    print('Validation RMSE: ', rmse,'\n')

    return rmse,

population_size = 4
num_generations = 4
gene_length = 10

# As we are trying to minimize the RMSE score, that's why using -1.0.
# In case, when you want to maximize accuracy for instance, use 1.0
creator.create('FitnessMax', base.Fitness, weights = (-1.0,))
creator.create('Individual', list , fitness = creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register('binary', bernoulli.rvs, 0.5)
toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary,
n = gene_length)
toolbox.register('population', tools.initRepeat, list , toolbox.individual)

toolbox.register('mate', tools.cxOrdered)
toolbox.register('mutate', tools.mutShuffleIndexes, indpb = 0.6)
toolbox.register('select', tools.selRoulette)
toolbox.register('evaluate', train_evaluate)

population = toolbox.population(n = population_size)
r = algorithms.eaSimple(population, toolbox, cxpb = 0.3, mutpb = 0.1,
ngen = num_generations, verbose = False)
# Print top N solutions - (1st only, for now)
best_individuals = tools.selBest(population,k = 1)
best_window_size = None
best_num_units = None

for bi in best_individuals:
    window_size_bits = BitArray(bi[0:6])
    num_units_bits = BitArray(bi[6:])
    best_window_size = window_size_bits.uint
    best_num_units = num_units_bits.uint
    print('\nWindow Size: ', best_window_size, ', Num of Units: ', best_num_units)
from keras import metrics

trainX1,trainY1 = prepare(train_data,best_window_size,1)
testX1, testY1 = prepare(test_data,best_window_size,1)

from keras.regularizers import l1, l2
from keras import regularizers
def l1_l2(l1=0.01, l2=0.01):
    return regularizers.l1_l2(l1=l1, l2=l2)

#print(testY.shape)
import tensorflow as tf
from tensorflow import keras

model = Sequential()


model.add(LSTM(best_num_units, input_shape=(trainX1.shape[1],trainX1.shape[2])))

model.add(Dense(3))



model.add(Dense(1))

import tensorflow as tf
import keras.backend as K
from sklearn.metrics import r2_score
def r_squared(y_true, y_pred):
    SS_res =  K.sum(K.square(y_true - y_pred))
    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )

# Définir la fonction de métrique rmse
def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true)))


def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true)))

#une couche de sortie qui fait une prédiction à valeur unique
model.compile(loss=root_mean_squared_error, optimizer="adam",metrics=[r_squared, root_mean_squared_error])


history=model.fit(trainX1,trainY1, epochs=400, batch_size=21,verbose=2,validation_data=(testX1,testY1))





print(len(model.layers))
model.summary()
model.input_shape
model.output_shape
model.input_names
model.layers
from sklearn.metrics import mean_squared_error
# make predictions
#trainPredict = model.predict(X_test1)
import math
yhat = model.predict(testX1)
r_squared_train = history.history['root_mean_squared_error']
r_squared_val = history.history['val_root_mean_squared_error']
epochs = range(1, len(r_squared_train) + 1)
plt.plot(epochs, r_squared_train, label='Training root_mean_squared_error')
plt.plot(epochs, r_squared_val, label='Validation root_mean_squared_error')
plt.title('Training and validation root_mean_squared_error')
plt.legend()
#plt. savefig("r_squaredfigure")

plt.show()

##################

scaler=MinMaxScaler()
print(yhat.shape)

mse=mean_squared_error(yhat[:,0], testY1[:,0])
RMSE = math.sqrt(mse)

#testscore = math.sqrt(mean_squared_error(yhat, testY))
from matplotlib import pyplot
print('test Score sans renf: %.4f RMSE' % (RMSE))
from sklearn.metrics import mean_absolute_error
Mae=mean_absolute_error(yhat[:,0], testY1[:,0])
print('test Score: %.2f MAE sans renf' % (Mae))
from sklearn.metrics import r2_score
r2=r2_score(yhat[:,0], testY1[:,0])
print('test Score: %.2f R2 sans renf' % (r2))
plt.plot(history.history['loss'])
plt.title('RMSE Curve')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.show()

plt.plot([1], [ r2], label='Train R-squared')

plt.legend()
plt.title('R-squared Curve')
plt.xlabel('Epoch')
plt.ylabel('R-squared')
plt.show()

yhat1=yhat
pyplot.plot(history.history['loss'])
pyplot.plot(history.history['accuracy'])
pyplot.legend(['loss','accuracy'],loc='upper right')
plt.plot(yhat[:,0])
plt.plot(testY1[:,0])
plt.show()
df1 = pd.DataFrame(data=testY1[:,0], columns=['testY'])
df2 = pd.DataFrame(data=yhat[:,0], columns=['yhat'])
df3 = pd.merge(df1, df2, left_index=True, right_index=True)
df3.head()
df3.tail()
forecast_errors = [testY1[i,0]-yhat[i,0] for i in range(len(testY1))]
print('Forecast Errors: %s' % forecast_errors)
for i in range(len(forecast_errors)):
  print('Forecast Errors: %s' % forecast_errors[i])
loss=history.history['loss']
val_los=history.history['val_loss']
epochs=range(1,len(loss)+1)
plt.plot(epochs,loss,'y',label='training loss',color='red')
plt.plot(epochs,val_los,'y',label='validation loss')
plt.title('training and validation loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()
pred = np.array(yhat[:,0].flatten()).tolist()
true = np.array(testY1[:,0].flatten()).tolist()


print(pred)
print(true)
missClassified = []
index_yhat=[]

for i in range(len(pred)):
   x=pred[i]-true[i]
   if (abs(x)>0.1):
      missClassified.append(i)
      index_yhat.append(missClassified[i])

   else:
      missClassified.append(-1)
print(len(missClassified))
print(missClassified)
print(len(index_yhat))
s=0
newtestx=[]
newtesty=[]
for i in range(len(missClassified)):
    if (missClassified[i]!=-1):
       s=s+1
       newtestx.append(testX1[i,0:5])
       newtesty.append(testY1[i,0])
print(len(newtestx))
print(newtestx)
print(len(newtesty))
print(len(pred))
print(len(true))
pred1=np.array(newtestx).tolist()
pred1
pred2=np.array(newtesty).tolist()
newtestx_tab=np.array(newtestx)
newtesty_tab=np.array(newtesty)
newtesty_tab.shape
newtestx_tab.shape
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_reg = PolynomialFeatures()
X_poly = poly_reg.fit_transform(newtestx_tab[:,0]) # transformation de tab_X_train en polynome
pol_reg2 = LinearRegression()
pol_reg2.fit(X_poly, newtesty_tab)
yhatpol=pol_reg2.predict(X_poly)
plt.plot(yhatpol)
plt.show()
newtesty_tab.shape
newtestx_tab.shape
plt.plot(newtesty_tab)
plt.show()
msepol=mean_squared_error(yhatpol, newtesty_tab)
RMSEpol = math.sqrt(msepol)

#testscore = math.sqrt(mean_squared_error(yhat, testY))
from matplotlib import pyplot
print('test Score: %.2f RMSEpol' % (RMSEpol))
print(newtestx_tab.shape)
newtesty_tab.shape
newtestx_tab.shape
yhatnex=np.append(yhat,yhatpol)
print(len(yhatnex))
def my_function(x):
  return list(dict.fromkeys(x))
mylist = my_function(yhatnex)
print(len(mylist))

print(len(index_yhat))
print(len(yhatpol))
len1=len(yhatpol)
print(len1)
len2=len(yhat)
print(len2)
print(yhatpol)
for i in range(len1):
  yhat[index_yhat[i]]=yhatpol[i]
print(len(yhat))
mse3=mean_squared_error(yhat[:,0], testY1[:,0])
RMSE3 = math.sqrt(mse3)

#testscore = math.sqrt(mean_squared_error(yhat, testY))
from matplotlib import pyplot
print('test Score: %.2f RMSE apres renf' % (RMSE3))
print("R2-score: coefficient de détermination apres renf=", r2_score(yhat[:,0], testY1[:,0]) )
Maesvm=mean_absolute_error(yhat[:,0], testY1[:,0])
print('test Score: %.2f mae apres renf' % (Maesvm))

df4 = pd.DataFrame(data=testY1[:,0], columns=['yhat'])
df5 = pd.DataFrame(data=yhat[:,0], columns=['yhat1'])
df6 = pd.merge(df4, df5, left_index=True, right_index=True)

print(index_yhat)















